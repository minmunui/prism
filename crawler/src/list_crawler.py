"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ í¬ë¡¤ëŸ¬ (Selenium ì—†ì´)
requestsë§Œ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ìˆ˜ì§‘
"""

from datetime import datetime
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import time


class NaverNewsSectionCrawler:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ í¬ë¡¤ëŸ¬"""

    # ì„¹ì…˜ ì½”ë“œ
    SECTIONS = {
        "ì •ì¹˜": "100",
        "ê²½ì œ": "101",
        "ì‚¬íšŒ": "102",
        "ìƒí™œ/ë¬¸í™”": "103",
        "ì„¸ê³„": "104",
        "IT/ê³¼í•™": "105",
        "ì˜¤í”¼ë‹ˆì–¸": "110",
    }

    def __init__(self, timeout: int = 10):
        """
        Args:
            timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            }
        )

    def fetch_page(self, section_id: str = None, page: int = 1, target_date: Optional[str] = None) -> Optional[List[Dict]]:
        """
        íŠ¹ì • ì„¹ì…˜ì˜ íŠ¹ì • í˜ì´ì§€ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°

        Args:
            section_id: ì„¹ì…˜ ì½”ë“œ (ì˜ˆ: '100' ë˜ëŠ” 'ì •ì¹˜')
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            target_date: íŠ¹ì • ë‚ ì§œ(YYYYMMDD)ë¡œ í•„í„°ë§

        Returns:
            ê¸°ì‚¬ ëª©ë¡ ë˜ëŠ” None
        """
        # ì„¹ì…˜ëª…ì„ ì½”ë“œë¡œ ë³€í™˜
        if section_id in self.SECTIONS:
            section_id = self.SECTIONS[section_id]

        url = "https://news.naver.com/main/list.naver"
        params = {
            "mode": "LSD",
            "mid": "shm",
            "page": page,
            "date": target_date if target_date else datetime.now().strftime('%Y%m%d'),
        }

        if section_id:
            params["sid1"] = section_id

        try:
            response = self.session.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            articles = self._parse_articles(soup)

            return articles

        except requests.RequestException as e:
            print(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None

    def _parse_articles(self, soup: BeautifulSoup) -> List[Dict]:
        """ê¸°ì‚¬ ëª©ë¡ íŒŒì‹±"""
        articles = []

        # í—¤ë“œë¼ì¸ê³¼ ì¼ë°˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ ì„ íƒ
        for dl in soup.select("#main_content > div.list_body.newsflash_body > ul > li > dl"):
            
            
            img_src = dl.select_one("dt.photo img")['src'] if dl.select_one("dt.photo img") else None
            title = dl.select_one("dt:not(.photo) a").text if dl.select_one("dt:not(.photo) a") else None
            link = dl.select_one("dt:not(.photo) a").get("href", "") if dl.select_one("dt:not(.photo) a") else None
            press = dl.select_one("dd span.writing").text if dl.select_one("dd span.writing") else ""
            date = dl.select_one("dd span.date").text if dl.select_one("dd span.date") else ""
            description = dl.select_one("dd span.lede").text if dl.select_one("dd span.lede") else ""

            articles.append(
                {
                    "img_src": img_src,
                    "title": title,
                    "link": link,
                    "press": press,
                    "date": date,
                    "description": description,
                }
            )

        return articles

    def fetch_multiple_pages(
        self, section_id: str = None, max_pages: int = 10, target_date=None, delay: float = 0.5
    ) -> List[Dict]:
        """
        ì—¬ëŸ¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ë¥¼ í•œ ë²ˆì— ìˆ˜ì§‘

        Args:
            section_id: ì„¹ì…˜ ì½”ë“œ (ì˜ˆ: '100' ë˜ëŠ” 'ì •ì¹˜')
            max_pages: ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜
            target_date: íŠ¹ì • ë‚ ì§œ(YYYYMMDD)ë¡œ í•„í„°ë§ (Noneì´ë©´ ì˜¤ëŠ˜ ë‚ ì§œ)
            delay: í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ì„œë²„ ë¶€ë‹´ ë°©ì§€)

        Returns:
            ëª¨ë“  ê¸°ì‚¬ ëª©ë¡
        """
        all_articles = []

        if target_date is None:
            target_date = datetime.now().strftime('%Y%m%d')
            print(f"ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì„¤ì •í•©ë‹ˆë‹¤. {target_date}")

        last_articles = []
        for page in range(1, max_pages + 1):
            print(f"í˜ì´ì§€ {page}/{max_pages} ìˆ˜ì§‘ ì¤‘...", end=" ")

            articles = self.fetch_page(section_id, page, target_date=target_date)


            if last_articles and articles == last_articles:
                print("âš ï¸ ì¤‘ë³µ ê¸°ì‚¬ ë°œê²¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                break

            last_articles = articles

            if articles:
                all_articles.extend(articles)
                print(f"âœ“ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                
            else:
                print("âœ— ìˆ˜ì§‘ ì‹¤íŒ¨")
                break

            # ì„œë²„ ë¶€ë‹´ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            if page < max_pages:
                time.sleep(delay)

        # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
        seen = set()
        unique_articles = []
        for article in all_articles:
            if article["link"] not in seen:
                seen.add(article["link"])
                unique_articles.append(article)

        print(f"\nì´ {len(unique_articles)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return unique_articles

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    crawler = NaverNewsSectionCrawler()

    # ë°©ë²• 1: í˜ì´ì§€ ë‹¨ìœ„ë¡œ ìˆ˜ì§‘
    print("\n[ ë°©ë²• 1 ] ì •ì¹˜ ì„¹ì…˜ 5í˜ì´ì§€ ìˆ˜ì§‘")
    print("-" * 60)
    articles = crawler.fetch_multiple_pages(
        section_id="ì •ì¹˜", max_pages=100  # ë˜ëŠ” '100'
    )

    print("\nì²˜ìŒ 10ê°œ ê¸°ì‚¬:")
    for i, article in enumerate(articles[:10], 1):
        print(f"\n{i}. {article['title']}")
        print(f"   ğŸ“° {article['press']} | â° {article['date']}")
        if article["description"]:
            print(f"   ğŸ’¬ {article['description'][:50]}...")
        print(f"   ğŸ”— {article['link']}")

    with open("naver_economy_news.json", "w", encoding="utf-8") as f:
        import json

        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"\nìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(articles)}ê°œ")
